{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "531433ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import KFold\n",
    "from PIL import Image\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "from torch.optim import AdamW\n",
    "\n",
    "os.environ['KAGGLE_KERNEL_RUN_TYPE'] = 'Batch'\n",
    "DEBUG = False\n",
    "TRAIN = True\n",
    "LOCAL = False\n",
    "\n",
    "tqdm.pandas()\n",
    "\n",
    "DATA_ROOT = '/kaggle/input/csiro-biomass/'\n",
    "\n",
    "train_df = pd.read_csv(f'{DATA_ROOT}/train.csv')\n",
    "print(f\"Données chargées : {len(train_df)} lignes\")\n",
    "print(train_df.head())\n",
    "\n",
    "train_df[['sample_id_prefix', 'sample_id_suffix']] = train_df.sample_id.str.split('__', expand=True)\n",
    "assert (train_df.sample_id_suffix == train_df.target_name).all(), \"Erreur : identifiants ne correspondent pas\"\n",
    "\n",
    "cols = ['sample_id_prefix', 'image_path', 'Sampling_Date', 'State', 'Species', 'Pre_GSHH_NDVI', 'Height_Ave_cm']\n",
    "agg_train_df = train_df.groupby(cols).apply(\n",
    "    lambda df: df.set_index('target_name').target,\n",
    "    include_groups=False\n",
    ")\n",
    "agg_train_df.reset_index(inplace=True)\n",
    "agg_train_df.columns.name = None\n",
    "\n",
    "print(\"Chargement des images en mémoire...\")\n",
    "agg_train_df['image'] = agg_train_df.image_path.progress_apply(\n",
    "    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n",
    ")\n",
    "print(f\"{len(agg_train_df)} images chargées\")\n",
    "print(agg_train_df['image'].apply(lambda x: x.size).value_counts())\n",
    "\n",
    "assert np.isclose(\n",
    "    agg_train_df[['Dry_Green_g', 'Dry_Clover_g']].sum(axis=1),\n",
    "    agg_train_df['GDM_g'], atol=1e-04\n",
    ").mean() > 0.99, \"Erreur : GDM ≠ Green + Clover\"\n",
    "\n",
    "assert np.isclose(\n",
    "    agg_train_df[['GDM_g', 'Dry_Dead_g']].sum(axis=1),\n",
    "    agg_train_df['Dry_Total_g'], atol=1e-04\n",
    ").mean() > 0.99, \"Erreur : Total ≠ GDM + Dead\"\n",
    "\n",
    "print(\"Relations entre cibles vérifiées\")\n",
    "\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 3, 1)\n",
    "agg_train_df.Dry_Green_g.plot(kind='hist', bins=50, color='green')\n",
    "plt.title('Végétation verte sèche (g)')\n",
    "plt.subplot(1, 3, 2)\n",
    "agg_train_df.Dry_Clover_g.plot(kind='hist', bins=50, color='lightgreen')\n",
    "plt.title('Trèfle sec (g)')\n",
    "plt.subplot(1, 3, 3)\n",
    "agg_train_df.Dry_Dead_g.plot(kind='hist', bins=50, color='brown')\n",
    "plt.title('Matière morte sèche (g)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('distribution_cibles.png')\n",
    "plt.show()\n",
    "\n",
    "NFOLD = 5\n",
    "kfold = KFold(n_splits=NFOLD, shuffle=True, random_state=42)\n",
    "\n",
    "agg_train_df['fold'] = -1\n",
    "for i, (trn_idx, val_idx) in enumerate(kfold.split(agg_train_df.index)):\n",
    "    agg_train_df.loc[val_idx, 'fold'] = i\n",
    "\n",
    "print(f\"Données découpées en {NFOLD} folds\")\n",
    "print(agg_train_df['fold'].value_counts().sort_index())\n",
    "\n",
    "class DatasetPaturage(Dataset):\n",
    "    def __init__(self, data, transform=None):\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data.iloc[idx]\n",
    "        image = item.image\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        cibles = [item['Dry_Green_g'], item['Dry_Clover_g'], item['Dry_Dead_g']]\n",
    "        return image, cibles\n",
    "\n",
    "def creer_dataloader(data, taille_image=(256, 256), batch_size=32, melanger=True, augmentation=True):\n",
    "    if augmentation:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(taille_image),\n",
    "            transforms.RandomHorizontalFlip(p=0.5),\n",
    "            transforms.RandomVerticalFlip(p=0.5),\n",
    "            transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.05),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    else:\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize(taille_image),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "\n",
    "    dataset = DatasetPaturage(data, transform=transform)\n",
    "    print(f'Taille du dataset : {len(dataset)} échantillons')\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=melanger,\n",
    "        num_workers=2,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "NOM_MODELE = 'efficientnet_b2'\n",
    "model_test = timm.create_model(NOM_MODELE, pretrained=True, num_classes=3)\n",
    "TAILLE_IMAGE_CIBLE = model_test.pretrained_cfg['input_size'][1:]\n",
    "print(f\"Modèle {NOM_MODELE} — Taille d'entrée : {TAILLE_IMAGE_CIBLE}\")\n",
    "del model_test\n",
    "\n",
    "def r2_pondere(y_true: np.ndarray, y_pred: np.ndarray):\n",
    "    poids = np.array([0.1, 0.1, 0.1, 0.2, 0.5])\n",
    "    scores_r2 = []\n",
    "    for i in range(5):\n",
    "        y_t = y_true[:, i]\n",
    "        y_p = y_pred[:, i]\n",
    "        ss_res = np.sum((y_t - y_p) ** 2)\n",
    "        ss_tot = np.sum((y_t - np.mean(y_t)) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot if ss_tot > 0 else 0.0\n",
    "        scores_r2.append(r2)\n",
    "    scores_r2 = np.array(scores_r2)\n",
    "    return np.sum(scores_r2 * poids) / np.sum(poids), scores_r2\n",
    "\n",
    "def calculer_metrique(sorties, cibles):\n",
    "    y_true = np.column_stack((cibles, cibles[:, :2].sum(axis=1), cibles.sum(axis=1)))\n",
    "    y_pred = np.column_stack((sorties, sorties[:, :2].sum(axis=1), sorties.sum(axis=1)))\n",
    "    return r2_pondere(y_true, y_pred)\n",
    "\n",
    "def entrainer_epoch(model, dataloader, critere, optimiseur, device):\n",
    "    model.train()\n",
    "    perte_totale = 0\n",
    "    for images, cibles in dataloader:\n",
    "        images = images.to(device)\n",
    "        cibles = torch.stack(cibles).T.float().to(device)\n",
    "        optimiseur.zero_grad()\n",
    "        sorties = model(images)\n",
    "        perte = critere(sorties, cibles)\n",
    "        perte.backward()\n",
    "        optimiseur.step()\n",
    "        perte_totale += perte.item()\n",
    "    return perte_totale / len(dataloader)\n",
    "\n",
    "def valider(model, dataloader, critere, device):\n",
    "    model.eval()\n",
    "    perte_totale = 0\n",
    "    toutes_sorties, toutes_cibles = [], []\n",
    "    with torch.no_grad():\n",
    "        for images, cibles in dataloader:\n",
    "            images = images.to(device)\n",
    "            cibles = torch.stack(cibles).T.float().to(device)\n",
    "            sorties = model(images)\n",
    "            perte = critere(sorties, cibles)\n",
    "            perte_totale += perte.item()\n",
    "            toutes_sorties.append(sorties.detach().cpu())\n",
    "            toutes_cibles.append(cibles.detach().cpu())\n",
    "    sorties_np = torch.cat(toutes_sorties).numpy()\n",
    "    cibles_np = torch.cat(toutes_cibles).numpy()\n",
    "    r2_val, scores_r2 = calculer_metrique(sorties_np, cibles_np)\n",
    "    return perte_totale / len(dataloader), r2_val, scores_r2\n",
    "\n",
    "def entrainer_fold(data, fold):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Dispositif : {device}\")\n",
    "\n",
    "    batch_size = 32\n",
    "    lr = 1e-3\n",
    "    patience = 10\n",
    "    nb_epochs = 100\n",
    "\n",
    "    model = timm.create_model(NOM_MODELE, pretrained=True, num_classes=3)\n",
    "    model.to(device)\n",
    "    critere = nn.SmoothL1Loss()\n",
    "    optimiseur = AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimiseur, mode='max', factor=0.5, patience=patience // 2\n",
    "    )\n",
    "\n",
    "    train_loader = creer_dataloader(data[data.fold != fold], TAILLE_IMAGE_CIBLE, batch_size, melanger=True, augmentation=True)\n",
    "    val_loader = creer_dataloader(data[data.fold == fold], TAILLE_IMAGE_CIBLE, batch_size, melanger=False, augmentation=False)\n",
    "\n",
    "    historique = []\n",
    "    meilleur_score = -float('inf')\n",
    "    epochs_sans_amelioration = 0\n",
    "\n",
    "    for epoch in range(nb_epochs):\n",
    "        perte_train = entrainer_epoch(model, train_loader, critere, optimiseur, device)\n",
    "        perte_val, r2_val, scores_r2 = valider(model, val_loader, critere, device)\n",
    "        scheduler.step(r2_val)\n",
    "\n",
    "        lr_actuel = optimiseur.param_groups[0]['lr']\n",
    "        print(f\"Epoch [{epoch:3d}/{nb_epochs}] | Train: {perte_train:.4f} | Val: {perte_val:.4f} | R²: {r2_val:.4f} | LR: {lr_actuel:.6f}\")\n",
    "\n",
    "        historique.append({\n",
    "            'perte_train': perte_train,\n",
    "            'perte_val': perte_val,\n",
    "            'r2_pondere': r2_val,\n",
    "            'scores_r2': scores_r2.tolist(),\n",
    "        })\n",
    "\n",
    "        if r2_val > meilleur_score:\n",
    "            meilleur_score = r2_val\n",
    "            epochs_sans_amelioration = 0\n",
    "            torch.save(model.state_dict(), f'{DOSSIER_SORTIE}/meilleur_modele_fold{fold}.pth')\n",
    "            print(f\"Meilleur modèle sauvegardé (R²={meilleur_score:.4f})\")\n",
    "        else:\n",
    "            epochs_sans_amelioration += 1\n",
    "\n",
    "        if epochs_sans_amelioration >= patience:\n",
    "            print(f\"Arrêt anticipé epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "    print(f\"\\nFold {fold} terminé — Meilleur R² : {meilleur_score:.4f}\")\n",
    "    return historique, meilleur_score\n",
    "\n",
    "DOSSIER_SORTIE = 'modeles_entraines/'\n",
    "os.makedirs(DOSSIER_SORTIE, exist_ok=True)\n",
    "\n",
    "if TRAIN:\n",
    "    tous_meilleurs_scores = []\n",
    "    print(\"Début de l'entraînement K-Fold...\\n\")\n",
    "\n",
    "    for i in range(NFOLD):\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        print(f\"FOLD {i}/{NFOLD-1}\")\n",
    "        print(f\"{'='*50}\")\n",
    "\n",
    "        historique, meilleur_score = entrainer_fold(agg_train_df, fold=i)\n",
    "        tous_meilleurs_scores.append(meilleur_score)\n",
    "\n",
    "        historique_df = pd.DataFrame(historique)\n",
    "        historique_df.to_json(f'{DOSSIER_SORTIE}/historique_fold{i}.jsonl', orient='records', lines=True, force_ascii=False)\n",
    "\n",
    "        plt.figure(figsize=(12, 4))\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(f'Perte — Fold {i}')\n",
    "        plt.plot(historique_df.perte_train, label='Entraînement')\n",
    "        plt.plot(historique_df.perte_val, label='Validation')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Perte (SmoothL1)')\n",
    "        plt.legend()\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(f'R² Pondéré — Fold {i}')\n",
    "        plt.plot(historique_df.r2_pondere, color='green')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('R²')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{DOSSIER_SORTIE}/courbes_fold{i}.png')\n",
    "        plt.show()\n",
    "    \n",
    "    for i, score in enumerate(tous_meilleurs_scores):\n",
    "        print(f\"Fold {i} : R² = {score:.4f}\")\n",
    "    print(f\"Moyenne CV : {np.mean(tous_meilleurs_scores):.4f} ± {np.std(tous_meilleurs_scores):.4f}\")\n",
    "\n",
    "def recuperer_derniers_modeles():\n",
    "    model_root = '/kaggle/input/csiro-simple-output/pytorch/default/'\n",
    "    latest = 1\n",
    "    for version in os.listdir(model_root):\n",
    "        try:\n",
    "            v = int(version)\n",
    "            if v > latest:\n",
    "                latest = v\n",
    "        except:\n",
    "            continue\n",
    "    return f'{model_root}/{latest}/modeles_entraines/'\n",
    "\n",
    "MODELES_SAUVEGARDES = DOSSIER_SORTIE if TRAIN else recuperer_derniers_modeles()\n",
    "\n",
    "def predire(model, dataloader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    toutes_sorties = []\n",
    "    with torch.no_grad():\n",
    "        for images, cibles in dataloader:\n",
    "            images = images.to(device)\n",
    "            sorties = model(images)\n",
    "            toutes_sorties.append(sorties.detach().cpu())\n",
    "    return torch.cat(toutes_sorties).numpy()\n",
    "\n",
    "def predire_ensemble(dataloader):\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    toutes_predictions = []\n",
    "\n",
    "    fichiers = sorted(Path(MODELES_SAUVEGARDES).glob('*.pth'))\n",
    "    if not fichiers:\n",
    "        raise FileNotFoundError(f\"Aucun modèle .pth trouvé dans {MODELES_SAUVEGARDES}\")\n",
    "\n",
    "    for fichier_modele in fichiers:\n",
    "        print(f\"Chargement : {fichier_modele.name}\")\n",
    "        model = timm.create_model(NOM_MODELE, pretrained=False, num_classes=3)\n",
    "        model.load_state_dict(\n",
    "            torch.load(fichier_modele, map_location='cpu', weights_only=True)\n",
    "        )\n",
    "        preds = predire(model, dataloader, device)\n",
    "        toutes_predictions.append(preds)\n",
    "\n",
    "    predictions_moyennes = np.mean(toutes_predictions, axis=0)\n",
    "    print(f\"Ensemble de {len(toutes_predictions)} modèles — prédictions moyennées\")\n",
    "    return predictions_moyennes\n",
    "\n",
    "print(\"\\nChargement des données de test...\")\n",
    "test_df = pd.read_csv(DATA_ROOT + 'test.csv')\n",
    "test_df['target'] = 0.0\n",
    "test_df[['sample_id_prefix', 'sample_id_suffix']] = test_df.sample_id.str.split('__', expand=True)\n",
    "\n",
    "cols_test = ['sample_id_prefix', 'image_path']\n",
    "agg_test_df = test_df.groupby(cols_test).apply(\n",
    "    lambda df: df.set_index('target_name').target,\n",
    "    include_groups=False\n",
    ")\n",
    "agg_test_df.reset_index(inplace=True)\n",
    "agg_test_df.columns.name = None\n",
    "\n",
    "print(\"Chargement des images de test...\")\n",
    "agg_test_df['image'] = agg_test_df.image_path.progress_apply(\n",
    "    lambda path: Image.open(DATA_ROOT + path).convert('RGB')\n",
    ")\n",
    "print(f\"{len(agg_test_df)} images de test chargées\")\n",
    "\n",
    "test_loader = creer_dataloader(agg_test_df, TAILLE_IMAGE_CIBLE, 32, melanger=False, augmentation=False)\n",
    "\n",
    "print(\"\\nGénération des prédictions...\")\n",
    "predictions = predire_ensemble(test_loader)\n",
    "\n",
    "agg_test_df[['Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g']] = predictions\n",
    "agg_test_df['GDM_g'] = agg_test_df.Dry_Green_g + agg_test_df.Dry_Clover_g\n",
    "agg_test_df['Dry_Total_g'] = agg_test_df.GDM_g + agg_test_df.Dry_Dead_g\n",
    "\n",
    "print(\"\\nAperçu des prédictions :\")\n",
    "print(agg_test_df[['sample_id_prefix', 'Dry_Green_g', 'Dry_Clover_g', 'Dry_Dead_g', 'GDM_g', 'Dry_Total_g']].head())\n",
    "\n",
    "cols_cibles = ['Dry_Clover_g', 'Dry_Dead_g', 'Dry_Green_g', 'Dry_Total_g', 'GDM_g']\n",
    "sub_df = agg_test_df.set_index('sample_id_prefix')[cols_cibles].stack()\n",
    "sub_df = sub_df.reset_index()\n",
    "sub_df.columns = ['sample_id_prefix', 'target_name', 'target']\n",
    "sub_df['sample_id'] = sub_df.sample_id_prefix + '__' + sub_df.target_name\n",
    "\n",
    "sub_df[['sample_id', 'target']].to_csv('submission.csv', index=False)\n",
    "print(\"\\nFichier submission.csv généré avec succès !\")\n",
    "print(sub_df[['sample_id', 'target']].head(10).to_string())"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
